{
  "timestamp": "2025-06-12 16:55:47",
  "reference_check": {
    "total_references": 5,
    "total_citations": 10,
    "unused_references": [],
    "missing_references": [],
    "citation_statistics": {
      "[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (pp. 5998-6008).": 6,
      "[2] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 4171-4186).": 1,
      "[3] Goldberg, Y. (2017). Neural network methods for natural language processing. Synthesis Lectures on Human Language Technologies, 10(1), 1-309.": 1,
      "[4] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems (pp. 3111-3119).": 1,
      "[5] Kim, Y. (2014). Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (pp. 1746-1751).": 1
    },
    "document_metadata": {
      "title": "未知标题",
      "author": "python-docx",
      "created": "2013-12-23 23:15:00+00:00",
      "modified": "2013-12-23 23:15:00+00:00"
    }
  }
}